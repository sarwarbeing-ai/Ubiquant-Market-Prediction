{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIghtgbmModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMdMQHY1d9R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/sarwar/lightgbm_model"
      ],
      "metadata": {
        "id": "X_HbvZiY1j4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import GroupKFold,KFold,StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "import pickle\n",
        "from IPython.display import FileLink\n",
        "import random\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation,BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "pd.set_option('display.max_columns',500)\n",
        "pd.set_option('display.max_rows',100)"
      ],
      "metadata": {
        "id": "YsFTPu981msn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=2022\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "os.environ['PYTHONHASHSEED']=str(seed)"
      ],
      "metadata": {
        "id": "o9SxDBXp1mqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data_path=\"/content/drive/MyDrive/sarwar/\"\n",
        "train=pd.read_pickle(os.path.join(data_path,\"train.pickle\"))"
      ],
      "metadata": {
        "id": "L6L51MQB1mm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "6Dajl3Wg1mj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train shape:\",train.shape)"
      ],
      "metadata": {
        "id": "AFpKCYzZ1mgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 300\n",
        "features = [f'f_{i}' for i in range(n_features)]"
      ],
      "metadata": {
        "id": "s1eAtKk-1mds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric\n",
        "def pearson_correlation(df):\n",
        "    corr=df.groupby('time_id')[['target','prediction']].corr().unstack().iloc[:,1].mean()\n",
        "    return corr"
      ],
      "metadata": {
        "id": "v7Eez4JV1map"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o5jECJPS1mX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "investment_id=train['investment_id']"
      ],
      "metadata": {
        "id": "Bmz9ltYs1mVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20\n",
        "BATCH_SIZE=1024\n",
        "LR=0.01\n",
        "VERBOSE=2"
      ],
      "metadata": {
        "id": "SKPNrUDn1mSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "investment_ids = list(investment_id.unique())\n",
        "investment_id_size = len(investment_ids) + 1\n",
        "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
        "with tf.device(\"cpu\"):\n",
        "    investment_id_lookup_layer.adapt(investment_id)"
      ],
      "metadata": {
        "id": "mj6LkaEX1mQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {  'task':'train',\n",
        "                'max_depth':10,\n",
        "                'min_gain_to_split':0.01, # 0.0 --->0.01\n",
        "                'min_sum_hessian_in_leaf':1e-2,\n",
        "               'force_col_wise':'true',\n",
        "               'objective':'regression',\n",
        "               'boosting':'dart',\n",
        "               'feature_fraction': 0.75,\n",
        "               'metric': ['mse','rmse','l1'],\n",
        "               'num_threads':-1, \n",
        "               'extra_trees':'true',\n",
        "               'extra_seed':7,  \n",
        "               'min_data_in_leaf': 300, \n",
        "               'bagging_fraction': 0.75, \n",
        "               'learning_rate': 0.1, \n",
        "               'bagging_seed': seed, \n",
        "               'num_leaves': 100, # 80 --> 100\n",
        "               'bagging_freq':5,  # 1 ---> 5\n",
        "               'lambda_l1':0.5,\n",
        "               'lambda_l2':1,\n",
        "               'drop_rate':0.3,\n",
        "               'xgboost_dart_mode':'true',\n",
        "               'max_bin':200,\n",
        "               'min_data_in_bin':40,\n",
        "                'path_smooth':10**(-2),\n",
        "                 'verbosity':-1\n",
        "              }"
      ],
      "metadata": {
        "id": "QP0tUAOA1mLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [f'f_{i}' for i in range(n_features)]\n",
        "# Initiate GroupKFold\n",
        "#kfold = GroupKFold(n_splits = 5)\n",
        "# Create groups based on time_id\n",
        "#train.loc[(train['time_id'] >= 0) & (train['time_id'] < 280), 'group'] = 0\n",
        "#train.loc[(train['time_id'] >= 280) & (train['time_id'] < 585), 'group'] = 1\n",
        "#train.loc[(train['time_id'] >= 585) & (train['time_id'] < 825), 'group'] = 2\n",
        "#train.loc[(train['time_id'] >= 825) & (train['time_id'] < 1030), 'group'] = 3\n",
        "#train.loc[(train['time_id'] >= 1030) & (train['time_id'] < 1400), 'group'] = 4\n",
        "#train['group'] = train['group'].astype(np.int16)\n",
        "kfold=KFold(n_splits=10,shuffle=False)\n",
        "# Store out of folds predictions\n",
        "oof_predictions = np.zeros(len(train))\n",
        "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
        "    print(f'Training fold {fold + 1}')\n",
        "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "    y_train, y_val = train['target'].iloc[trn_ind], train['target'].iloc[val_ind]\n",
        "    n_training_rows = x_train.shape[0]\n",
        "    n_validation_rows = x_val.shape[0]\n",
        "    # Build lgbm dataset\n",
        "    train_set= lgb.Dataset(x_train.values, y_train.values.ravel(),free_raw_data=True)\n",
        "    raw_data=None\n",
        "    gc.collect()\n",
        "    val_set=lgb.Dataset(x_val.values, y_val.values.ravel(),free_raw_data=True)\n",
        "    raw_data=None\n",
        "    gc.collect()\n",
        "    del x_train,y_train\n",
        "    gc.collect()\n",
        "    print(f'Training with {n_training_rows} rows')\n",
        "    print(f'Validating with {n_validation_rows} rows')\n",
        "    print(f'Training dart boosting model with {len(features)} features...')\n",
        "    # Train and evaluate\n",
        "    model = lgb.train(\n",
        "            params, \n",
        "            train_set, \n",
        "            num_boost_round = 160, # 150--->160\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=20)],\n",
        "            valid_sets = [train_set, val_set], \n",
        "            verbose_eval = 50,\n",
        "        valid_names=['train','valid']\n",
        "        )\n",
        "    # Predict validation set\n",
        "    val_pred = model.predict(x_val.values)\n",
        "    # Add validation prediction to out of folds array\n",
        "    oof_predictions[val_ind] = val_pred\n",
        "    # Save model to disk for inference\n",
        "    joblib.dump(model, f'/content/drive/MyDrive/sarwar/lightgbm_model/lgbm_{fold + 1}.pkl')\n",
        "    del x_val,y_val,train_set, val_set,model\n",
        "    gc.collect()\n",
        "# Compute out of folds Pearson Correlation Coefficient (for each time_id)\n",
        "oof_df = pd.DataFrame({'time_id': train['time_id'], 'target': train['target'], 'prediction': oof_predictions})\n",
        "# Save out of folds csv for blending\n",
        "oof_df.to_csv('/content/drive/MyDrive/sarwar/lightgbm_model/simple_lgbm.csv', index = False)\n",
        "score = pearson_correlation(oof_df)\n",
        "print(f'Our out of folds mean pearson correlation coefficient is {score}')  "
      ],
      "metadata": {
        "id": "fppmtjgl1mG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCQVjPIK1lhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasModel3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMdMQHY1d9R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/sarwar/model3"
      ],
      "metadata": {
        "id": "X_HbvZiY1j4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import GroupKFold,KFold,StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "import pickle\n",
        "from IPython.display import FileLink\n",
        "import random\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation,BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "pd.set_option('display.max_columns',500)\n",
        "pd.set_option('display.max_rows',100)"
      ],
      "metadata": {
        "id": "YsFTPu981msn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=2022\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "os.environ['PYTHONHASHSEED']=str(seed)"
      ],
      "metadata": {
        "id": "o9SxDBXp1mqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data_path=\"/content/drive/MyDrive/sarwar/\"\n",
        "train=pd.read_pickle(os.path.join(data_path,\"train.pickle\"))"
      ],
      "metadata": {
        "id": "L6L51MQB1mm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "6Dajl3Wg1mj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train shape:\",train.shape)"
      ],
      "metadata": {
        "id": "AFpKCYzZ1mgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 300\n",
        "features = [f'f_{i}' for i in range(n_features)]"
      ],
      "metadata": {
        "id": "s1eAtKk-1mds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric\n",
        "def pearson_correlation(df):\n",
        "    corr=df.groupby('time_id')[['target','prediction']].corr().unstack().iloc[:,1].mean()\n",
        "    return corr"
      ],
      "metadata": {
        "id": "v7Eez4JV1map"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o5jECJPS1mX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "investment_id=train['investment_id']"
      ],
      "metadata": {
        "id": "Bmz9ltYs1mVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20\n",
        "BATCH_SIZE=1024\n",
        "LR=0.01\n",
        "VERBOSE=2"
      ],
      "metadata": {
        "id": "SKPNrUDn1mSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "investment_ids = list(investment_id.unique())\n",
        "investment_id_size = len(investment_ids) + 1\n",
        "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
        "with tf.device(\"cpu\"):\n",
        "    investment_id_lookup_layer.adapt(investment_id)"
      ],
      "metadata": {
        "id": "mj6LkaEX1mQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X,y):\n",
        "  return X,y\n",
        "def make_dataset(feature, investment_id, y, batch_size=BATCH_SIZE, mode=\"train\"):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
        "    ds=ds.map(preprocess)\n",
        "    if mode == \"train\":\n",
        "        ds = ds.shuffle(256)\n",
        "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "a3u4Cay61mOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model3(n_features=300):\n",
        "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
        "    features_inputs = tf.keras.Input((n_features, ), dtype=tf.float32)\n",
        "    \n",
        "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
        "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
        "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
        "    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
        "    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    \n",
        "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
        "    feature_x = layers.Dropout(0.5)(feature_x)\n",
        "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dropout(0.5)(feature_x)\n",
        "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
        "    \n",
        "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
        "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
        "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(LR), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
        "    return model"
      ],
      "metadata": {
        "id": "QP0tUAOA1mLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(nfolds=5):\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = KFold(n_splits =nfolds,shuffle=False )\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train['target'].iloc[trn_ind], train['target'].iloc[val_ind]\n",
        "        investment_id_train=investment_id.iloc[trn_ind]\n",
        "        investment_id_val=investment_id.iloc[val_ind]\n",
        "        ds_train=make_dataset(x_train, investment_id_train.values,y_train.values, batch_size=BATCH_SIZE, mode=\"train\")\n",
        "        ds_val= make_dataset(x_val,investment_id_val.values, y_val.values, batch_size=BATCH_SIZE, mode=\"test\")\n",
        "        # Reset keras session\n",
        "        K.clear_session()\n",
        "        n_training_rows = x_train.shape[0]\n",
        "        n_validation_rows = x_val.shape[0]\n",
        "        print('Building model...')\n",
        "        model = model3(len(features))\n",
        "        print(f'Training with {n_training_rows} rows')\n",
        "        print(f'Validating with {n_validation_rows} rows')\n",
        "        print(f'Training model with {len(features)+1} features...')\n",
        "        # Callbacks\n",
        "        #callback=tf.keras.callbacks.EarlyStopping(\n",
        "        #          monitor=\"val_loss\",\n",
        "        #          min_delta=0,\n",
        "        #          patience=4,\n",
        "        #          verbose=1,\n",
        "        #          mode=\"min\",\n",
        "        #          baseline=None,\n",
        "        #          restore_best_weights=False, \n",
        "        #)\n",
        "        checkpoint_path=f\"/content/drive/MyDrive/sarwar/model3/fold_{fold+1}/cp.ckpt\"\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_path, \n",
        "            monitor = 'val_loss', \n",
        "            verbose = VERBOSE, \n",
        "            save_best_only = True,\n",
        "            save_weights_only = True, \n",
        "            mode = 'min', \n",
        "            save_freq = 'epoch'\n",
        "        )\n",
        "        # Train and evaluate\n",
        "        history = model.fit(\n",
        "            ds_train,\n",
        "            epochs = EPOCHS,\n",
        "            verbose = VERBOSE,\n",
        "            callbacks = [checkpoint],\n",
        "            validation_data = ds_val,\n",
        "        )\n",
        "        # Load model\n",
        "        model.load_weights(checkpoint_path)\n",
        "        # Predict validation set\n",
        "        val_pred = model.predict(ds_val).reshape(-1)\n",
        "        # Add validation prediction to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        del x_train, x_val, y_train, y_val,model,ds_train,ds_val\n",
        "        gc.collect()\n",
        "    # Compute out of folds Pearson Correlation Coefficient (for each time_id)\n",
        "    oof_df = pd.DataFrame({'time_id': train['time_id'], 'target': train['target'], 'prediction': oof_predictions})\n",
        "    # Save out of folds csv for blending\n",
        "    oof_df.to_csv('/content/drive/MyDrive/sarwar/model3/model_3_pred.csv', index = False)\n",
        "    score = pearson_correlation(oof_df)\n",
        "    print(f'Our out of folds mean pearson correlation coefficient is {score}')\n",
        "    return oof_df\n",
        "    \n",
        "oof_df=train_and_evaluate(10)"
      ],
      "metadata": {
        "id": "fppmtjgl1mG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCQVjPIK1lhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}